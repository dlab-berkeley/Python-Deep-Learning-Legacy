{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Note you may get a warning about CUDA and GPU set up\n",
    "# You can ignore these for now\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data(subset=True):\n",
    "    \"\"\"\n",
    "    Returns the MNIST dataset as a tuple:\n",
    "    (x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "    \n",
    "    When subset=TRUE:\n",
    "    Returns only a subset of the mnist dataset.\n",
    "    Especially important to use if you are on datahub and only have 1-2GB of memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    if subset:\n",
    "        N_TRAIN = 5000\n",
    "        N_VALIDATION = 1000\n",
    "        N_TEST = 1000\n",
    "    else:\n",
    "        N_TRAIN = 48000\n",
    "        N_VALIDATION = 12000\n",
    "        N_TEST = 10000\n",
    "    \n",
    "    (x_train_and_val, y_train_and_val), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    x_train = x_train_and_val[:N_TRAIN,:,:]\n",
    "    y_train = y_train_and_val[:N_TRAIN]\n",
    "    \n",
    "    x_val = x_train_and_val[N_TRAIN: N_TRAIN + N_VALIDATION,:,:]\n",
    "    y_val = y_train_and_val[N_TRAIN: N_TRAIN + N_VALIDATION]\n",
    "    \n",
    "    x_test = x_test[:N_TEST]\n",
    "    y_test = y_test[:N_TEST]\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "# Load the data\n",
    "# Set subset=False if you want to use the full dataset!\n",
    "# Note that this will require 2+ GB of memory and will make training take longer\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_mnist_data(subset=True)\n",
    "\n",
    "def transform_data(xdata, ydata):\n",
    "    \"\"\"\n",
    "    Transforms image data:\n",
    "        1. Flattens pixel dimensions from 2 -> 1\n",
    "        2. Scales pixel values between [0,1]\n",
    "    Transforms target data (ydata):\n",
    "        - Formats targets as one hot encoded columns\n",
    "    \"\"\"\n",
    "    \n",
    "    x = {}\n",
    "    for name, partition in zip([\"x_train\", \"x_val\", \"x_test\"],xdata):\n",
    "        flatten = partition.reshape((partition.shape[0], 28 * 28))\n",
    "        scaled = flatten.astype('float32') / 255\n",
    "        x[name] = scaled\n",
    "    \n",
    "    y = {}\n",
    "    for name, partition in zip([\"y_train\", \"y_val\", \"y_test\"],ydata):\n",
    "        y[name] = to_categorical(partition)\n",
    "    \n",
    "    return x['x_train'], y['y_train'], x['x_val'], y['y_val'], x['x_test'], y['y_test']\n",
    "\n",
    "\n",
    "x_train_trans, y_train_trans, x_val_trans, y_val_trans, x_test_trans, y_test_trans = transform_data([x_train, x_val, x_test],\n",
    "                                                                                                    [y_train, y_val, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Understanding the Input Data\n",
    "\n",
    "1. Why do we use split our data into train/validation/test?\n",
    "2. What is the shape of our input data partitions?\n",
    "3. What is the type of the data?\n",
    "\n",
    "**BONUS:**\n",
    "\n",
    "4. What is the distribution of the target classes within the data, is it balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Why do we split our data into train, validation, and test sets?\n",
    "\n",
    "# We need to train our model and avoid overfitting.\n",
    "\n",
    "# We use the training set to fit the model.\n",
    "# We use the validation set to tune the hyperparameters of our model.\n",
    "# We use the holdout test set to determine our final performance (generalization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 What is the shape of our input data partitions?\n",
    "\n",
    "# The training set contains 5000 examples\n",
    "# The validation set contains 1000 examples \n",
    "# The test set contains 1000 examples\n",
    "# The X are 3-dimensional\n",
    "# The y are 1 dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3\n",
    "[type(partition) for partition in [x_train, y_train, x_val, y_val, x_test, y_test]]\n",
    "# All are numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS 1.4\n",
    "\n",
    "# Simply plot the histogram of each y in a different cell, for example\n",
    "# plt.hist(y_train)\n",
    "# plt.title('Train Class Distribution');\n",
    "# repeat for y_val and y_test\n",
    " \n",
    "# or make subplots\n",
    "def plot_target_distributions(targets, titles):\n",
    "    \"\"\"\n",
    "    Returns the distribution of target classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3,1, figsize = (10,10))\n",
    "    \n",
    "    for ax, target, title in zip(axes, targets, titles):\n",
    "        ax.hist(target) \n",
    "        ax.set_title(f\"{title} Class Distribution\")\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "plot_target_distributions([y_train, y_val, y_test], [\"Train\", \"Validation\", \"Test\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Build your own neural network\n",
    "\n",
    "1. Build and compile your own neural network in an object called `my_network`. Feel free to choose your own:\n",
    "    - Architecture\n",
    "    - Activation Function\n",
    "    - Epochs\n",
    "    \n",
    "2. Train your model, saving the results to an object called `history_my_network`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1\n",
    "# An example network with: 3 dense layers, each with 512 neurons and a dropout of 0.3\n",
    "\n",
    "my_network = Sequential()\n",
    "my_network.add(Dense(512, activation= \"relu\", input_shape=(28*28,)))\n",
    "my_network.add(Dropout(0.3))\n",
    "my_network.add(Dense(512, activation= \"relu\"))\n",
    "my_network.add(Dropout(0.3))\n",
    "my_network.add(Dense(512, activation= \"relu\"))\n",
    "my_network.add(Dropout(0.3))\n",
    "my_network.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "my_network.compile(optimizer = 'rmsprop', \n",
    "                     loss = 'categorical_crossentropy',\n",
    "                     metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2\n",
    "# Train model for 20 epochs with batch size of 128\n",
    "history_my_network = my_network.fit(x_train_trans, \n",
    "                            y_train_trans, \n",
    "                            epochs=20, \n",
    "                            batch_size=128, \n",
    "                            validation_data=(x_val_trans, y_val_trans))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: Evaluate your own model\n",
    "\n",
    "Use your own model from challenge 2 to evaluate its general performance.\n",
    "\n",
    "1. Visualize the training and validation accuracy over each epoch.\n",
    "2. Print the accuracy of your model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epoch_accuracy(history_dict):\n",
    "    \"\"\"\n",
    "    Plots the training and validation accuracy of a neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    acc = history_dict['accuracy']\n",
    "    val_acc = history_dict['val_accuracy']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, color = 'navy', alpha = 0.8, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, color = 'green', label='Validation Accuracy')\n",
    "    plt.title('Training and validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    return plt.show()\n",
    "\n",
    "def get_model_accuracy(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Takes a model and a test set of data.\n",
    "    Returns the accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    accuracy = round(score[1]*100, 1)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 \n",
    "plot_epoch_accuracy(history_my_network.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2\n",
    "get_model_accuracy(my_network, x_test_trans, y_test_trans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "9179d63e122556c6756b68a7ef958f74b99cc51ba44ce56116c8db8491528148"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
