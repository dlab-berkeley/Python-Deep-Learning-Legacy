{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a87e8f0",
   "metadata": {},
   "source": [
    "# Comparing Performance of Different Types of Neural Networks\n",
    "\n",
    "**Time**\n",
    "- Teaching: 15 minutes\n",
    "\n",
    "**Questions**\n",
    "- \"How do vanilla neural networks compare to convolutional neural networks?\"\n",
    "\n",
    "* * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e673336",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "The neural networks we have created so far are known as *vanilla neural networks* also known as *fully-connected, feed-foward neural networks*. \n",
    "\n",
    "These have many great usecases, but for problems in computer vision, we often use a different architecture called covolutional neural networks (CNNS).\n",
    "\n",
    "We will review the the details of how the work in the slides, but for now let's just compare their efficacy in image classification to vanilla neural nets by:\n",
    "1. Loading a vanilla neural network we used in the last notebook\n",
    "2. Building a convolutional neural network\n",
    "3. Comparing the classification accuracy between the two models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034bb61e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de327f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c2a7e",
   "metadata": {},
   "source": [
    "### Input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data(subset=True):\n",
    "    \"\"\"\n",
    "    Returns the MNIST dataset as a tuple:\n",
    "    (x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "    \n",
    "    When subset=TRUE:\n",
    "    Returns only a subset of the mnist dataset.\n",
    "    Especially important to use if you are on datahub and only have 1-2GB of memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    if subset:\n",
    "        N_TRAIN = 5000\n",
    "        N_VALIDATION = 1000\n",
    "        N_TEST = 1000\n",
    "    else:\n",
    "        N_TRAIN = 48000\n",
    "        N_VALIDATION = 12000\n",
    "        N_TEST = 10000\n",
    "    \n",
    "    (x_train_and_val, y_train_and_val), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    x_train = x_train_and_val[:N_TRAIN,:,:]\n",
    "    y_train = y_train_and_val[:N_TRAIN]\n",
    "    \n",
    "    x_val = x_train_and_val[N_TRAIN: N_TRAIN + N_VALIDATION,:,:]\n",
    "    y_val = y_train_and_val[N_TRAIN: N_TRAIN + N_VALIDATION]\n",
    "    \n",
    "    x_test = x_test[:N_TEST]\n",
    "    y_test = y_test[:N_TEST]\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c261f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = get_mnist_data(subset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30230c81",
   "metadata": {},
   "source": [
    "### Transformation to one dimension\n",
    "\n",
    "This is the same transformation we used in the last notebook to flatten our image pixels to one dimension for use in a vanilla neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a18d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_dim_transform_data(xdata, ydata):\n",
    "    \"\"\"\n",
    "    Transforms image data:\n",
    "        1. Flattens pixel dimensions from 2 -> 1\n",
    "        2. Scales pixel values between [0,1]\n",
    "    Transforms target data (ydata):\n",
    "        - Formats targets as one hot encoded columns\n",
    "    \"\"\"\n",
    "    \n",
    "    x = {}\n",
    "    for name, partition in zip([\"x_train\", \"x_val\", \"x_test\"],xdata):\n",
    "        flatten = partition.reshape((partition.shape[0], 28 * 28))\n",
    "        scaled = flatten.astype('float32') / 255\n",
    "        x[name] = scaled\n",
    "    \n",
    "    y = {}\n",
    "    for name, partition in zip([\"y_train\", \"y_val\", \"y_test\"],ydata):\n",
    "        y[name] = to_categorical(partition)\n",
    "    \n",
    "    return x['x_train'], y['y_train'], x['x_val'], y['y_val'], x['x_test'], y['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_trans, y_train_trans, x_val_trans, y_val_trans, x_test_trans, y_test_trans = one_dim_transform_data([x_train, x_val, x_test],\n",
    "                                                                                                    [y_train, y_val, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808ecb1",
   "metadata": {},
   "source": [
    "### Loading saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc952d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = os.path.join(\"..\", \"data\", \"third_nn\")\n",
    "\n",
    "vanilla_model = load_model(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e9864",
   "metadata": {},
   "source": [
    "### Transformation back to two dimensions\n",
    "While vanilla neural networks primarily handle one-dimensional input data, convolutional neural networks work well on multidimensional input data!\n",
    "\n",
    "We will backtransform our 1-dimensional data into 2-dimensions.\n",
    "\n",
    "*Note* - We also must add a depth/channel dimension to our data. Color pictures have 3 channels for Red, Green, and Blue while our black and white mnist images only have 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_transform_2d(data):\n",
    "    \"\"\"\n",
    "    Takes a list of flattened input pixel data.\n",
    "    Reshapes pixel data from a single vector to two dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    two_dimensional_data = []\n",
    "    \n",
    "    for d in data:\n",
    "        # reshape to [index for image, pixel row, pixel column, channels]\n",
    "        transformed = d.reshape(d.shape[0], 28, 28, 1)\n",
    "        two_dimensional_data.append(transformed)\n",
    "    \n",
    "    return [t for t in two_dimensional_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c002f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2d, x_val_2d, x_test_2d = back_transform_2d([x_train_trans, x_val_trans, x_test_trans])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917387c3",
   "metadata": {},
   "source": [
    "Let's confirm that we succesfully reshaped our data back to 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48699c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08eae10",
   "metadata": {},
   "source": [
    "Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c142d96",
   "metadata": {},
   "source": [
    "### Convolutional neural network\n",
    "\n",
    "Ignore the details regarding implementation for now, just know we are building a rather small convolutional neural network here to compare with our vanilla neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ec69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = Sequential()\n",
    "\n",
    "convnet.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "convnet.add(MaxPooling2D((2, 2)))\n",
    "convnet.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "convnet.add(MaxPooling2D((2, 2)))\n",
    "convnet.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(64, activation='relu'))\n",
    "convnet.add(Dense(10, activation='softmax'))\n",
    "\n",
    "convnet.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdf466",
   "metadata": {},
   "source": [
    "### Comparing architectures and number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f0b09",
   "metadata": {},
   "source": [
    "Notice any interesting differences between the two model architectures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b23d7d",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e481e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet_history = convnet.fit(x_train_2d,\n",
    "                      y_train_trans, \n",
    "                      epochs=10,\n",
    "                      batch_size=64,\n",
    "                      validation_data=(x_val_2d, y_val_trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ad920",
   "metadata": {},
   "source": [
    "### Accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epoch_accuracy(history_dict):\n",
    "    \"\"\"\n",
    "    Plots the training and validation accuracy of a neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    acc = history_dict['accuracy']\n",
    "    val_acc = history_dict['val_accuracy']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, color = 'navy', alpha = 0.8, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, color = 'green', label='Validation Accuracy')\n",
    "    plt.title('Training and validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epoch_accuracy(convnet_history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbba5a7",
   "metadata": {},
   "source": [
    "### Accuracy on the test data\n",
    "So how well does our CNN perform on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa409212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Takes a model and a test set of data.\n",
    "    Returns the accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    accuracy = round(score[1]*100, 1)\n",
    "    \n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c31c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_accuracy = get_model_accuracy(vanilla_model, x_test_trans, y_test_trans)\n",
    "convnet_accuracy = get_model_accuracy(convnet,x_test_2d, y_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classification accuracy results: \\n\\nVanilla Neural Network: {vanilla_accuracy}%\\nConvolution Neural Network: {convnet_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08352551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wrong_predictions(model, x_test, y_test, title = \"\"):\n",
    "    \"\"\"\n",
    "    Plots 16 incorrectly predicted images.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Back transform images\n",
    "    x_images = x_test.reshape(x_test.shape[0], 28, 28)\n",
    "    \n",
    "    # Format predictions and targets\n",
    "    predictions = model.predict(x_test)\n",
    "    predicted = np.argmax(predictions, axis=1)\n",
    "    target = np.argmax(y_test, axis = 1)\n",
    "    \n",
    "    # Get wrong indices\n",
    "    wrong_indices = np.where(predicted != target)[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(4,4, figsize = (30,30))\n",
    "    fig.suptitle(title, fontsize=30)\n",
    "    \n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for ax, index in zip(axes, wrong_indices[:17]):\n",
    "        ax.imshow(x_images[index], cmap=plt.cm.binary, interpolation='nearest')\n",
    "        ax.set_title(f\"Predicted {predicted[index]}, Actual is {target[index]}\", size = 25)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0357ddc",
   "metadata": {},
   "source": [
    "### Comparing wrong predictions\n",
    "\n",
    "Let's visualize incorrect wrong predictions between these two models and see if we can get some insight into how reasonable these mistakes are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wrong_predictions(vanilla_model, x_test_trans, y_test_trans, title = \"Wrong Predictions in Vanilla Neural Networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c477ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wrong_predictions(convnet, x_test_2d, y_test_trans, title = \"Wrong Predictions in Convolutional Neural Networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13dced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
